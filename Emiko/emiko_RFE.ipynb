{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# Recursive Feature Elimination \n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('../Data/training_set_features.csv', index_col=\"respondent_id\")\n",
    "labels_df = pd.read_csv('../Data/training_set_labels.csv', index_col=\"respondent_id\")\n",
    "joined_df = features_df.join(labels_df, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an all-in-one data cleaning function. Do this BEFORE OHE\n",
    "# Maybe this should be a class and worked into the pipeline?\n",
    "def datacleaner(maindataframe):\n",
    "    #For dropping whole columns \n",
    "    def columndrop(dataframe, column_list):\n",
    "        dataframe.drop(column_list, axis = 1, inplace=True)\n",
    "    #For dropping rows with na values\n",
    "    def basicdropna(dataframe, column_list):\n",
    "        dataframe.dropna(subset=column_list, inplace=True)\n",
    "    #For special case imputation\n",
    "    def impute_missing_data(dataframe, column_list, fillvalue):\n",
    "        for column in column_list:\n",
    "            dataframe[column].fillna(fillvalue, inplace = True)\n",
    "    #This creates a number of lists of columns that fall into a few different \n",
    "    #categories, that will be processed in different ways. See notes below on how\n",
    "    #these choices were made.\n",
    "    drop_columns =  ['employment_industry',  'employment_occupation', 'hhs_geo_region']       \n",
    "        \n",
    "    general_dropna = ['health_worker', 'education','income_poverty', 'marital_status', \n",
    "                    'rent_or_own', 'employment_status', 'household_adults', \n",
    "                    'household_children' ]\n",
    "        \n",
    "    survey_col = ['opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc',\n",
    "         'opinion_seas_vacc_effective', 'opinion_seas_risk','opinion_seas_sick_from_vacc']\n",
    "\n",
    "    behavior_col = ['behavioral_antiviral_meds', 'behavioral_face_mask',\n",
    "                'behavioral_large_gatherings','behavioral_outside_home']\n",
    "\n",
    "    behavior_col_2 = ['behavioral_avoidance', \n",
    "                'behavioral_wash_hands','behavioral_touch_face']\n",
    "\n",
    "    doc_rec = ['doctor_recc_h1n1','doctor_recc_seasonal']\n",
    "    \n",
    "    basicdropna(maindataframe, general_dropna)\n",
    "    columndrop(maindataframe, drop_columns)\n",
    "    impute_missing_data(maindataframe, survey_col, 3)\n",
    "    impute_missing_data(maindataframe, ['h1n1_concern'], 2)\n",
    "    impute_missing_data(maindataframe, ['h1n1_knowledge'], 0)\n",
    "    impute_missing_data(maindataframe, behavior_col, 0)\n",
    "    impute_missing_data(maindataframe, behavior_col_2, 1)\n",
    "    impute_missing_data(maindataframe, doc_rec, 0)\n",
    "    impute_missing_data(maindataframe, ['chronic_med_condition'], 0)\n",
    "    impute_missing_data(maindataframe, ['child_under_6_months'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaner(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=joined_df.drop(['h1n1_vaccine','seasonal_vaccine'], axis=1)\n",
    "y=joined_df[['h1n1_vaccine','seasonal_vaccine']]\n",
    "\n",
    "# Train test split, do this before OHE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OHE for objects, do this before imputer\n",
    "\n",
    "cat_col_list = [i for i in X_train.select_dtypes(include='object').columns]\n",
    "\n",
    "nb_list_for_ohe = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "'opinion_seas_risk', 'opinion_seas_sick_from_vacc']\n",
    "\n",
    "# Fits OHE on a subset of columns, then reintegrates them into the\n",
    "# Origional dataframe. Do this after initial cleaning, before \n",
    "# health insurace imputation.\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "def fit_trans_ohe(X_dataframe, columns):\n",
    "        \n",
    "    dums = ohe.fit_transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated\n",
    "\n",
    "#We should end up with a fitted ohe instance called 'ohe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = fit_trans_ohe(X_train, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "socio_economic_column_list = [\"x0_35 - 44 Years\",\"x0_45 - 54 Years\",\"x0_55 - 64 Years\",\"x0_65+ Years\",\n",
    "                              \"x1_< 12 Years\",\"x1_College Graduate\",\"x1_Some College\",\"x2_Hispanic\",\n",
    "                              \"x2_Other or Multiple\",\"x2_White\",\"x3_Male\", \"x4_> $75,000\", \"x4_Below Poverty\",\n",
    "                              \"x5_Not Married\", \"x6_Rent\", \"x7_Not in Labor Force\",\"x7_Unemployed\",\n",
    "                              \"x8_MSA, Principle City\",'x8_Non-MSA', 'health_insurance']\n",
    "\n",
    "# Fitting an imputer for Health Insurance using socio-economic features, \n",
    "# pulling from a dataframe that has already been OneHotEncoded\n",
    "\n",
    "\n",
    "soc_eco_h_i_imputer_knn = KNNImputer()\n",
    "\n",
    "def soc_eco_KNN_imputer(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.fit_transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round() \n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = soc_eco_KNN_imputer(soc_eco_h_i_imputer_knn, X_train_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OHE for the test set only, takes X test dataframe and list of columns to encoded:\n",
    "def trans_ohe(X_dataframe, columns):\n",
    "    dums = ohe.transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = trans_ohe(X_test, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer_transform_only(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed = imputer_transform_only(soc_eco_h_i_imputer_knn, X_test_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have a working dataset of: \n",
    "    'X_train_imputed' and 'y_train' to fit models to, 'X_test_ohe' to generate predictions, and 'y_test' to validate models with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding for non-binary features \n",
    "\n",
    "non_binary = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "'opinion_seas_risk', 'opinion_seas_sick_from_vacc']\n",
    "\n",
    "\n",
    "# X_test data\n",
    "nb_train = X_train_imputed[non_binary]\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "dums = ohe.fit_transform(nb_train)\n",
    "\n",
    "dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=nb_train.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_test data\n",
    "\n",
    "nb_test = X_test_ohe[non_binary]\n",
    "\n",
    "dums_t = ohe.transform(nb_test)\n",
    "\n",
    "dums_t_df = pd.DataFrame(dums_t,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=nb_test.index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat one hot encoded df and X \n",
    "\n",
    "# X_train data\n",
    "X_train = X_train_imputed.drop(non_binary, axis=1)\n",
    "\n",
    "X_train_imputed_ohe = pd.concat([X_train, dums_df], axis=1)\n",
    "\n",
    "# X_test data\n",
    "X_test = X_test_ohe.drop(non_binary, axis=1)\n",
    "\n",
    "X_test_ohe_2 = pd.concat([X_test, dums_t_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_iputed => some has more than 2 categories\n",
    "\n",
    "X_train_imputed_ohe => all binary \n",
    "\n",
    "\n",
    "X_test_ohe => some has more than 2 categories\n",
    "\n",
    "X_test_ohe_2 => all binary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.783521\n",
       "1    0.216479\n",
       "Name: h1n1_vaccine, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.h1n1_vaccine.value_counts(normalize=True) # =>change to DummyClassifier later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.531887\n",
       "1    0.468113\n",
       "Name: seasonal_vaccine, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.seasonal_vaccine.value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (default settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1N1 Cross-val ROC_AUC score:0.8392174922109827\n",
      "Seasonal Cross-val ROC_AUC score:0.8563820317814604\n",
      "H1N1 Cross-val ROC_AUC score (All Binary):0.8392343999877966\n",
      "Seasonal Cross-val ROC_AUC score (All Binary):0.856398673986693\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "### with X_train_imputed \n",
    "\n",
    "# H1N1 \n",
    "h1n1 = cross_val_score(estimator=lr, X=X_train_imputed, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal\n",
    "sea = cross_val_score(estimator=lr, X=X_train_imputed, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "\n",
    "### with X_train_imputed_ohe (all binary) \n",
    "\n",
    "\n",
    "# H1N1 \n",
    "h1n1_1 = cross_val_score(estimator=lr, X=X_train_imputed_ohe, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal\n",
    "sea_1 = cross_val_score(estimator=lr, X=X_train_imputed_ohe, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1N1 cv-mean ROC_AUC score:0.8392174922109827\n",
      "Seasonal cv-mean ROC_AUC score:0.8563820317814604\n",
      "H1N1 cv-mean ROC_AUC score (All Binary):0.8392343999877966\n",
      "Seasonal cv-mean ROC_AUC score (All Binary):0.856398673986693\n"
     ]
    }
   ],
   "source": [
    "# Results \n",
    "print(f'H1N1 cv-mean ROC_AUC score:{h1n1}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score:{sea}')\n",
    "\n",
    "print(f'H1N1 cv-mean ROC_AUC score (All Binary):{h1n1_1}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score (All Binary):{sea_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with best params (by grid search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1N1 cv-mean ROC_AUC score (All Binary/Best Params):0.8396002112901335\n",
      "Seasonal cv-mean ROC_AUC score (All Binary/Best Params):0.8564305668521636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# H1N1 \n",
    "# {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_h1n1=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "h1n1_bp = cross_val_score(estimator=logreg_h1n1, X=X_train_imputed_ohe, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal \n",
    "#{'C': 1.0, 'class_weight': 'balanced', 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_sea=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=10000,\n",
    "                              solver='saga')\n",
    "\n",
    "sea_bp = cross_val_score(estimator=logreg_sea, X=X_train_imputed_ohe, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "\n",
    "print(f'H1N1 cv-mean ROC_AUC score (All Binary/Best Params):{h1n1_bp}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score (All Binary/Best Params):{sea_bp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Feature Ranking with Recursive Feature Elimination, select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['behavioral_face_mask', 'behavioral_large_gatherings',\n",
       "       'doctor_recc_h1n1', 'doctor_recc_seasonal', 'child_under_6_months',\n",
       "       'health_worker', 'x9_3.0', 'x11_3.0', 'x11_4.0', 'x11_5.0', 'x12_2.0',\n",
       "       'x12_3.0', 'x12_4.0', 'x12_5.0', 'x13_2.0', 'x13_3.0', 'x14_3.0',\n",
       "       'x15_2.0', 'x15_3.0', 'x15_4.0', 'x15_5.0', 'x16_3.0', 'x16_4.0',\n",
       "       'x16_5.0', 'x0_55 - 64 Years', 'x0_65+ Years', 'x1_< 12 Years',\n",
       "       'x2_Hispanic', 'x2_Other or Multiple', 'x2_White', 'x3_Male',\n",
       "       'x5_Not Married', 'health_insurance', 'x2_3.0', 'x2_4.0', 'x2_5.0',\n",
       "       'x3_3.0', 'x3_4.0', 'x3_5.0', 'x4_3.0', 'x5_3.0', 'x6_3.0', 'x6_4.0',\n",
       "       'x6_5.0', 'x7_3.0', 'x7_5.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H1N1 \n",
    "\n",
    "# Use the model with all binary / best params \n",
    "model_for_RFE = logreg_h1n1=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "\n",
    "# Instantiate and fit the selector\n",
    "selector = RFE(model_for_RFE)\n",
    "selector.fit(X_train_imputed_ohe, y_train.h1n1_vaccine) \n",
    "\n",
    "# Print the results\n",
    "#print(\"Was the column selected?\")\n",
    "#for index, col in enumerate(X_train_imputed_ohe.columns):\n",
    "#    print(f\"{col}: {selector.support_[index]}\")\n",
    "\n",
    "    \n",
    "#print(selector.support_)\n",
    "# print(selector.ranking_)\n",
    "f = selector.get_support(1) #the most important features\n",
    "X_train_imputed_ohe.columns[f] # final features`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['behavioral_touch_face', 'doctor_recc_h1n1', 'doctor_recc_seasonal',\n",
       "       'health_worker', 'x10_2.0', 'x11_4.0', 'x11_5.0', 'x13_5.0', 'x14_3.0',\n",
       "       'x14_4.0', 'x14_5.0', 'x15_2.0', 'x15_3.0', 'x15_4.0', 'x15_5.0',\n",
       "       'x16_2.0', 'x16_3.0', 'x16_4.0', 'x16_5.0', 'x0_35 - 44 Years',\n",
       "       'x0_45 - 54 Years', 'x0_55 - 64 Years', 'x0_65+ Years', 'x1_< 12 Years',\n",
       "       'x1_College Graduate', 'x2_Hispanic', 'x2_Other or Multiple',\n",
       "       'x2_White', 'x4_Below Poverty', 'x7_Unemployed', 'health_insurance',\n",
       "       'x1_1.0', 'x1_2.0', 'x2_3.0', 'x5_2.0', 'x5_3.0', 'x5_4.0', 'x5_5.0',\n",
       "       'x6_2.0', 'x6_3.0', 'x6_4.0', 'x6_5.0', 'x7_2.0', 'x7_3.0', 'x7_4.0',\n",
       "       'x7_5.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seasonal Flu\n",
    "\n",
    "# Use the model with all binary / best params \n",
    "model_for_RFE_1 = logreg_sea=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=10000,\n",
    "                              solver='saga')\n",
    "\n",
    "# Instantiate and fit the selector\n",
    "selector_1 = RFE(model_for_RFE_1)\n",
    "selector_1.fit(X_train_imputed_ohe, y_train.seasonal_vaccine) \n",
    "\n",
    "# Print the results\n",
    "#print(\"Was the column selected?\")\n",
    "#for index, col in enumerate(X_train_imputed_ohe.columns):\n",
    "#    print(f\"{col}: {selector.support_[index]}\")\n",
    "\n",
    "    \n",
    "#print(selector.support_)\n",
    "# print(selector.ranking_)\n",
    "f_1 = selector_1.get_support(1) #the most important features\n",
    "X_train_imputed_ohe.columns[f_1] # final features`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
