{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# Recursive Feature Elimination \n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('../Data/training_set_features.csv', index_col=\"respondent_id\")\n",
    "labels_df = pd.read_csv('../Data/training_set_labels.csv', index_col=\"respondent_id\")\n",
    "joined_df = features_df.join(labels_df, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an all-in-one data cleaning function. Do this BEFORE OHE\n",
    "# Maybe this should be a class and worked into the pipeline?\n",
    "def datacleaner(maindataframe):\n",
    "    #For dropping whole columns \n",
    "    def columndrop(dataframe, column_list):\n",
    "        dataframe.drop(column_list, axis = 1, inplace=True)\n",
    "    #For dropping rows with na values\n",
    "    def basicdropna(dataframe, column_list):\n",
    "        dataframe.dropna(subset=column_list, inplace=True)\n",
    "    #For special case imputation\n",
    "    def impute_missing_data(dataframe, column_list, fillvalue):\n",
    "        for column in column_list:\n",
    "            dataframe[column].fillna(fillvalue, inplace = True)\n",
    "    #This creates a number of lists of columns that fall into a few different \n",
    "    #categories, that will be processed in different ways. See notes below on how\n",
    "    #these choices were made.\n",
    "    drop_columns =  ['employment_industry',  'employment_occupation', 'hhs_geo_region']       \n",
    "        \n",
    "    general_dropna = ['health_worker', 'education','income_poverty', 'marital_status', \n",
    "                    'rent_or_own', 'employment_status', 'household_adults', \n",
    "                    'household_children' ]\n",
    "        \n",
    "    survey_col = ['opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc',\n",
    "         'opinion_seas_vacc_effective', 'opinion_seas_risk','opinion_seas_sick_from_vacc']\n",
    "\n",
    "    behavior_col = ['behavioral_antiviral_meds', 'behavioral_face_mask',\n",
    "                'behavioral_large_gatherings','behavioral_outside_home']\n",
    "\n",
    "    behavior_col_2 = ['behavioral_avoidance', \n",
    "                'behavioral_wash_hands','behavioral_touch_face']\n",
    "\n",
    "    doc_rec = ['doctor_recc_h1n1','doctor_recc_seasonal']\n",
    "    \n",
    "    basicdropna(maindataframe, general_dropna)\n",
    "    columndrop(maindataframe, drop_columns)\n",
    "    impute_missing_data(maindataframe, survey_col, 3)\n",
    "    impute_missing_data(maindataframe, ['h1n1_concern'], 2)\n",
    "    impute_missing_data(maindataframe, ['h1n1_knowledge'], 0)\n",
    "    impute_missing_data(maindataframe, behavior_col, 0)\n",
    "    impute_missing_data(maindataframe, behavior_col_2, 1)\n",
    "    impute_missing_data(maindataframe, doc_rec, 0)\n",
    "    impute_missing_data(maindataframe, ['chronic_med_condition'], 0)\n",
    "    impute_missing_data(maindataframe, ['child_under_6_months'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaner(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=joined_df.drop(['h1n1_vaccine','seasonal_vaccine'], axis=1)\n",
    "y=joined_df[['h1n1_vaccine','seasonal_vaccine']]\n",
    "\n",
    "# Train test split, do this before OHE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OHE for objects, do this before imputer\n",
    "\n",
    "cat_col_list = [i for i in X_train.select_dtypes(include='object').columns]\n",
    "\n",
    "nb_list_for_ohe = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "'opinion_seas_risk', 'opinion_seas_sick_from_vacc']\n",
    "\n",
    "# Fits OHE on a subset of columns, then reintegrates them into the\n",
    "# Origional dataframe. Do this after initial cleaning, before \n",
    "# health insurace imputation.\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "def fit_trans_ohe(X_dataframe, columns):\n",
    "        \n",
    "    dums = ohe.fit_transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated\n",
    "\n",
    "#We should end up with a fitted ohe instance called 'ohe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = fit_trans_ohe(X_train, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "socio_economic_column_list = [\"x0_35 - 44 Years\",\"x0_45 - 54 Years\",\"x0_55 - 64 Years\",\"x0_65+ Years\",\n",
    "                              \"x1_< 12 Years\",\"x1_College Graduate\",\"x1_Some College\",\"x2_Hispanic\",\n",
    "                              \"x2_Other or Multiple\",\"x2_White\",\"x3_Male\", \"x4_> $75,000\", \"x4_Below Poverty\",\n",
    "                              \"x5_Not Married\", \"x6_Rent\", \"x7_Not in Labor Force\",\"x7_Unemployed\",\n",
    "                              \"x8_MSA, Principle City\",'x8_Non-MSA', 'health_insurance']\n",
    "\n",
    "# Fitting an imputer for Health Insurance using socio-economic features, \n",
    "# pulling from a dataframe that has already been OneHotEncoded\n",
    "\n",
    "\n",
    "soc_eco_h_i_imputer_knn = KNNImputer()\n",
    "\n",
    "def soc_eco_KNN_imputer(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.fit_transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round() \n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = soc_eco_KNN_imputer(soc_eco_h_i_imputer_knn, X_train_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OHE for the test set only, takes X test dataframe and list of columns to encoded:\n",
    "def trans_ohe(X_dataframe, columns):\n",
    "    dums = ohe.transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = trans_ohe(X_test, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer_transform_only(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed = imputer_transform_only(soc_eco_h_i_imputer_knn, X_test_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have a working dataset of: \n",
    "    'X_train_imputed' and 'y_train' to fit models to, 'X_test_ohe' to generate predictions, and 'y_test' to validate models with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding for non-binary features \n",
    "\n",
    "non_binary = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "'opinion_seas_risk', 'opinion_seas_sick_from_vacc']\n",
    "\n",
    "\n",
    "# X_test data\n",
    "nb_train = X_train_imputed[non_binary]\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "dums = ohe.fit_transform(nb_train)\n",
    "\n",
    "dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=nb_train.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_test data\n",
    "\n",
    "nb_test = X_test_ohe[non_binary]\n",
    "\n",
    "dums_t = ohe.transform(nb_test)\n",
    "\n",
    "dums_t_df = pd.DataFrame(dums_t,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=nb_test.index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat one hot encoded df and X \n",
    "\n",
    "# X_train data\n",
    "X_train = X_train_imputed.drop(non_binary, axis=1)\n",
    "\n",
    "X_train_imputed_ohe = pd.concat([X_train, dums_df], axis=1)\n",
    "\n",
    "# X_test data\n",
    "X_test = X_test_ohe.drop(non_binary, axis=1)\n",
    "\n",
    "X_test_ohe_2 = pd.concat([X_test, dums_t_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_iputed => some has more than 2 categories\n",
    "\n",
    "X_train_imputed_ohe => all binary \n",
    "\n",
    "\n",
    "X_test_ohe => some has more than 2 categories\n",
    "\n",
    "X_test_ohe_2 => all binary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.783521\n",
       "1    0.216479\n",
       "Name: h1n1_vaccine, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.h1n1_vaccine.value_counts(normalize=True) # =>change to DummyClassifier later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.531887\n",
       "1    0.468113\n",
       "Name: seasonal_vaccine, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.seasonal_vaccine.value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (default settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emikonaomasa/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "### with X_train_imputed \n",
    "\n",
    "# H1N1 \n",
    "h1n1 = cross_val_score(estimator=lr, X=X_train_imputed, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal\n",
    "sea = cross_val_score(estimator=lr, X=X_train_imputed, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "\n",
    "### with X_train_imputed_ohe (all binary) \n",
    "\n",
    "\n",
    "# H1N1 \n",
    "h1n1_1 = cross_val_score(estimator=lr, X=X_train_imputed_ohe, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal\n",
    "sea_1 = cross_val_score(estimator=lr, X=X_train_imputed_ohe, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1N1 cv-mean ROC_AUC score:0.8392174922109827\n",
      "Seasonal cv-mean ROC_AUC score:0.8563820317814604\n",
      "H1N1 cv-mean ROC_AUC score (All Binary):0.8392343999877966\n",
      "Seasonal cv-mean ROC_AUC score (All Binary):0.856398673986693\n"
     ]
    }
   ],
   "source": [
    "# Results \n",
    "print(f'H1N1 cv-mean ROC_AUC score:{h1n1}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score:{sea}')\n",
    "\n",
    "print(f'H1N1 cv-mean ROC_AUC score (All Binary):{h1n1_1}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score (All Binary):{sea_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with best params (by grid search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1N1 cv-mean ROC_AUC score (All Binary/Best Params):0.8395998352319072\n",
      "Seasonal cv-mean ROC_AUC score (All Binary/Best Params):0.8564304810629716\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# H1N1 \n",
    "# {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_h1n1=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "h1n1_bp = cross_val_score(estimator=logreg_h1n1, X=X_train_imputed_ohe, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal \n",
    "#{'C': 1.0, 'class_weight': 'balanced', 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_sea=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=10000,\n",
    "                              solver='saga')\n",
    "\n",
    "sea_bp = cross_val_score(estimator=logreg_sea, X=X_train_imputed_ohe, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "\n",
    "print(f'H1N1 cv-mean ROC_AUC score (All Binary/Best Params):{h1n1_bp}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score (All Binary/Best Params):{sea_bp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Feature Ranking with Recursive Feature Elimination, select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['behavioral_face_mask', 'behavioral_large_gatherings',\n",
       "       'doctor_recc_h1n1', 'doctor_recc_seasonal', 'child_under_6_months',\n",
       "       'health_worker', 'x9_3.0', 'x11_3.0', 'x11_4.0', 'x11_5.0', 'x12_2.0',\n",
       "       'x12_3.0', 'x12_4.0', 'x12_5.0', 'x13_2.0', 'x13_3.0', 'x14_3.0',\n",
       "       'x15_2.0', 'x15_3.0', 'x15_4.0', 'x15_5.0', 'x16_3.0', 'x16_4.0',\n",
       "       'x16_5.0', 'x0_55 - 64 Years', 'x0_65+ Years', 'x1_< 12 Years',\n",
       "       'x2_Hispanic', 'x2_Other or Multiple', 'x2_White', 'x3_Male',\n",
       "       'x5_Not Married', 'health_insurance', 'x2_3.0', 'x2_4.0', 'x2_5.0',\n",
       "       'x3_3.0', 'x3_4.0', 'x3_5.0', 'x4_3.0', 'x5_3.0', 'x6_3.0', 'x6_4.0',\n",
       "       'x6_5.0', 'x7_3.0', 'x7_5.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H1N1 \n",
    "\n",
    "# Use the model with all binary / best params \n",
    "model_for_RFE = LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "\n",
    "# Instantiate and fit the selector\n",
    "selector = RFE(model_for_RFE)\n",
    "selector.fit(X_train_imputed_ohe, y_train.h1n1_vaccine) \n",
    "\n",
    "# Print the results\n",
    "#print(\"Was the column selected?\")\n",
    "#for index, col in enumerate(X_train_imputed_ohe.columns):\n",
    "#    print(f\"{col}: {selector.support_[index]}\")\n",
    "\n",
    "    \n",
    "#print(selector.support_)\n",
    "# print(selector.ranking_)\n",
    "f = selector.get_support(1) #the most important features\n",
    "X_train_imputed_ohe.columns[f] # final features`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['behavioral_touch_face', 'doctor_recc_h1n1', 'doctor_recc_seasonal',\n",
       "       'health_worker', 'x10_2.0', 'x11_4.0', 'x11_5.0', 'x13_5.0', 'x14_2.0',\n",
       "       'x14_3.0', 'x14_4.0', 'x14_5.0', 'x15_2.0', 'x15_3.0', 'x15_4.0',\n",
       "       'x15_5.0', 'x16_2.0', 'x16_3.0', 'x16_4.0', 'x16_5.0',\n",
       "       'x0_35 - 44 Years', 'x0_45 - 54 Years', 'x0_55 - 64 Years',\n",
       "       'x0_65+ Years', 'x1_< 12 Years', 'x1_College Graduate', 'x2_Hispanic',\n",
       "       'x2_Other or Multiple', 'x2_White', 'x4_Below Poverty', 'x7_Unemployed',\n",
       "       'health_insurance', 'x1_1.0', 'x1_2.0', 'x2_3.0', 'x5_3.0', 'x5_4.0',\n",
       "       'x5_5.0', 'x6_2.0', 'x6_3.0', 'x6_4.0', 'x6_5.0', 'x7_2.0', 'x7_3.0',\n",
       "       'x7_4.0', 'x7_5.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seasonal Flu\n",
    "\n",
    "# Use the model with all binary / best params \n",
    "model_for_RFE_1 = LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=10000,\n",
    "                              solver='saga')\n",
    "\n",
    "# Instantiate and fit the selector\n",
    "selector_1 = RFE(model_for_RFE_1)\n",
    "selector_1.fit(X_train_imputed_ohe, y_train.seasonal_vaccine) \n",
    "\n",
    "# Print the results\n",
    "#print(\"Was the column selected?\")\n",
    "#for index, col in enumerate(X_train_imputed_ohe.columns):\n",
    "#    print(f\"{col}: {selector.support_[index]}\")\n",
    "\n",
    "    \n",
    "#print(selector.support_)\n",
    "# print(selector.ranking_)\n",
    "f_1 = selector_1.get_support(1) #the most important features\n",
    "X_train_imputed_ohe.columns[f_1] # final features`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavioral_antiviral_meds: 41\n",
      "behavioral_avoidance: 16\n",
      "behavioral_face_mask: 1\n",
      "behavioral_wash_hands: 22\n",
      "behavioral_large_gatherings: 1\n",
      "behavioral_outside_home: 36\n",
      "behavioral_touch_face: 15\n",
      "doctor_recc_h1n1: 1\n",
      "doctor_recc_seasonal: 1\n",
      "chronic_med_condition: 17\n",
      "child_under_6_months: 1\n",
      "health_worker: 1\n",
      "household_adults: 39\n",
      "household_children: 35\n",
      "x9_1.0: 45\n",
      "x9_2.0: 29\n",
      "x9_3.0: 1\n",
      "x10_1.0: 6\n",
      "x10_2.0: 23\n",
      "x11_2.0: 11\n",
      "x11_3.0: 1\n",
      "x11_4.0: 1\n",
      "x11_5.0: 1\n",
      "x12_2.0: 1\n",
      "x12_3.0: 1\n",
      "x12_4.0: 1\n",
      "x12_5.0: 1\n",
      "x13_2.0: 1\n",
      "x13_3.0: 1\n",
      "x13_4.0: 21\n",
      "x13_5.0: 47\n",
      "x14_2.0: 42\n",
      "x14_3.0: 1\n",
      "x14_4.0: 14\n",
      "x14_5.0: 3\n",
      "x15_2.0: 1\n",
      "x15_3.0: 1\n",
      "x15_4.0: 1\n",
      "x15_5.0: 1\n",
      "x16_2.0: 18\n",
      "x16_3.0: 1\n",
      "x16_4.0: 1\n",
      "x16_5.0: 1\n",
      "x0_35 - 44 Years: 24\n",
      "x0_45 - 54 Years: 32\n",
      "x0_55 - 64 Years: 1\n",
      "x0_65+ Years: 1\n",
      "x1_< 12 Years: 1\n",
      "x1_College Graduate: 9\n",
      "x1_Some College: 38\n",
      "x2_Hispanic: 1\n",
      "x2_Other or Multiple: 1\n",
      "x2_White: 1\n",
      "x3_Male: 1\n",
      "x4_> $75,000: 28\n",
      "x4_Below Poverty: 27\n",
      "x5_Not Married: 1\n",
      "x6_Rent: 43\n",
      "x7_Not in Labor Force: 25\n",
      "x7_Unemployed: 34\n",
      "x8_MSA, Principle City: 20\n",
      "x8_Non-MSA: 19\n",
      "health_insurance: 1\n",
      "x0_1.0: 44\n",
      "x0_2.0: 40\n",
      "x0_3.0: 13\n",
      "x1_1.0: 30\n",
      "x1_2.0: 31\n",
      "x2_2.0: 26\n",
      "x2_3.0: 1\n",
      "x2_4.0: 1\n",
      "x2_5.0: 1\n",
      "x3_2.0: 2\n",
      "x3_3.0: 1\n",
      "x3_4.0: 1\n",
      "x3_5.0: 1\n",
      "x4_2.0: 12\n",
      "x4_3.0: 1\n",
      "x4_4.0: 33\n",
      "x4_5.0: 46\n",
      "x5_2.0: 37\n",
      "x5_3.0: 1\n",
      "x5_4.0: 4\n",
      "x5_5.0: 8\n",
      "x6_2.0: 5\n",
      "x6_3.0: 1\n",
      "x6_4.0: 1\n",
      "x6_5.0: 1\n",
      "x7_2.0: 7\n",
      "x7_3.0: 1\n",
      "x7_4.0: 10\n",
      "x7_5.0: 1\n"
     ]
    }
   ],
   "source": [
    "for index, col in enumerate(X_train_imputed_ohe.columns):\n",
    "    print(f\"{col}: {selector.ranking_[index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41 16  1 22  1 36 15  1  1 17  1  1 39 35 45 29  1  6 23 11  1  1  1  1\n",
      "  1  1  1  1  1 21 47 42  1 14  3  1  1  1  1 18  1  1  1 24 32  1  1  1\n",
      "  9 38  1  1  1  1 28 27  1 43 25 34 20 19  1 44 40 13 30 31 26  1  1  1\n",
      "  2  1  1  1 12  1 33 46 37  1  4  8  5  1  1  1  7  1 10  1]\n"
     ]
    }
   ],
   "source": [
    "print(selector.ranking_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## race, health insurance -> important key features for the h1n1\n",
    "# poverty -> key for the flu  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Plot Predicted Probabiliy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1N1 cv-mean ROC_AUC score (All Binary/Best Params):0.8396002105500762\n",
      "Seasonal cv-mean ROC_AUC score (All Binary/Best Params):0.8564303096043213\n"
     ]
    }
   ],
   "source": [
    "# with logit with best setns of parameter variables, produce figures \n",
    "\n",
    "\n",
    "# H1N1 \n",
    "# {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_h1n1=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "h1n1_bp = cross_val_score(estimator=logreg_h1n1, X=X_train_imputed_ohe, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal \n",
    "#{'C': 1.0, 'class_weight': 'balanced', 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_sea=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=10000,\n",
    "                              solver='saga')\n",
    "\n",
    "sea_bp = cross_val_score(estimator=logreg_sea, X=X_train_imputed_ohe, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "\n",
    "print(f'H1N1 cv-mean ROC_AUC score (All Binary/Best Params):{h1n1_bp}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score (All Binary/Best Params):{sea_bp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', max_iter=1000000, penalty='l1',\n",
       "                   solver='saga')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_h1n1.fit(X_train_imputed_ohe,y_train.h1n1_vaccine )\n",
    "#preds_log = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2dad7cf694dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogreg_h1n1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_ohe_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1466\u001b[0m                                                 self.solver == 'liblinear')))\n\u001b[1;32m   1467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0movr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "logreg_h1n1.predict_proba(X_test_ohe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6559, 92)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ohe_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15304, 92)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imputed_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
