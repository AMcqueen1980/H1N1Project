{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('Data/training_set_features.csv', index_col=\"respondent_id\")\n",
    "labels_df = pd.read_csv('Data/training_set_labels.csv', index_col=\"respondent_id\")\n",
    "joined_df = features_df.join(labels_df, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an all-in-one data cleaning function. Do this BEFORE OHE\n",
    "# Maybe this should be a class and worked into the pipeline?\n",
    "def datacleaner(maindataframe):\n",
    "    #For dropping whole columns \n",
    "    def columndrop(dataframe, column_list):\n",
    "        dataframe.drop(column_list, axis = 1, inplace=True)\n",
    "    #For dropping rows with na values\n",
    "    def basicdropna(dataframe, column_list):\n",
    "        dataframe.dropna(subset=column_list, inplace=True)\n",
    "    #For special case imputation\n",
    "    def impute_missing_data(dataframe, column_list, fillvalue):\n",
    "        for column in column_list:\n",
    "            dataframe[column].fillna(fillvalue, inplace = True)\n",
    "    #This creates a number of lists of columns that fall into a few different \n",
    "    #categories, that will be processed in different ways. See notes below on how\n",
    "    #these choices were made.\n",
    "    drop_columns =  ['employment_industry',  'employment_occupation', 'hhs_geo_region']       \n",
    "        \n",
    "    general_dropna = ['health_worker', 'education','income_poverty', 'marital_status', \n",
    "                    'rent_or_own', 'employment_status', 'household_adults', \n",
    "                    'household_children' ]\n",
    "        \n",
    "    survey_col = ['opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc',\n",
    "         'opinion_seas_vacc_effective', 'opinion_seas_risk','opinion_seas_sick_from_vacc']\n",
    "\n",
    "    behavior_col = ['behavioral_antiviral_meds', 'behavioral_face_mask',\n",
    "                'behavioral_large_gatherings','behavioral_outside_home']\n",
    "\n",
    "    behavior_col_2 = ['behavioral_avoidance', \n",
    "                'behavioral_wash_hands','behavioral_touch_face']\n",
    "\n",
    "    doc_rec = ['doctor_recc_h1n1','doctor_recc_seasonal']\n",
    "    \n",
    "    basicdropna(maindataframe, general_dropna)\n",
    "    columndrop(maindataframe, drop_columns)\n",
    "    impute_missing_data(maindataframe, survey_col, 3)\n",
    "    impute_missing_data(maindataframe, ['h1n1_concern'], 2)\n",
    "    impute_missing_data(maindataframe, ['h1n1_knowledge'], 0)\n",
    "    impute_missing_data(maindataframe, behavior_col, 0)\n",
    "    impute_missing_data(maindataframe, behavior_col_2, 1)\n",
    "    impute_missing_data(maindataframe, doc_rec, 0)\n",
    "    impute_missing_data(maindataframe, ['chronic_med_condition'], 0)\n",
    "    impute_missing_data(maindataframe, ['child_under_6_months'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaner(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=joined_df.drop(['h1n1_vaccine','seasonal_vaccine'], axis=1)\n",
    "y=joined_df[['h1n1_vaccine','seasonal_vaccine']]\n",
    "\n",
    "# Train test split, do this before OHE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OHE for objects, do this before imputer\n",
    "\n",
    "cat_col_list = [i for i in X_train.select_dtypes(include='object').columns]\n",
    "\n",
    "nb_list_for_ohe = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "'opinion_seas_risk', 'opinion_seas_sick_from_vacc']\n",
    "\n",
    "# Fits OHE on a subset of columns, then reintegrates them into the\n",
    "# Origional dataframe. Do this after initial cleaning, before \n",
    "# health insurace imputation.\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "def fit_trans_ohe(X_dataframe, columns):\n",
    "        \n",
    "    dums = ohe.fit_transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated\n",
    "\n",
    "#We should end up with a fitted ohe instance called 'ohe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = fit_trans_ohe(X_train, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "socio_economic_column_list = [\"x0_35 - 44 Years\",\"x0_45 - 54 Years\",\"x0_55 - 64 Years\",\"x0_65+ Years\",\n",
    "                              \"x1_< 12 Years\",\"x1_College Graduate\",\"x1_Some College\",\"x2_Hispanic\",\n",
    "                              \"x2_Other or Multiple\",\"x2_White\",\"x3_Male\", \"x4_> $75,000\", \"x4_Below Poverty\",\n",
    "                              \"x5_Not Married\", \"x6_Rent\", \"x7_Not in Labor Force\",\"x7_Unemployed\",\n",
    "                              \"x8_MSA, Principle City\",'x8_Non-MSA', 'health_insurance']\n",
    "\n",
    "# Fitting an imputer for Health Insurance using socio-economic features, \n",
    "# pulling from a dataframe that has already been OneHotEncoded\n",
    "\n",
    "\n",
    "soc_eco_h_i_imputer_knn = KNNImputer()\n",
    "\n",
    "def soc_eco_KNN_imputer(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.fit_transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round() \n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = soc_eco_KNN_imputer(soc_eco_h_i_imputer_knn, X_train_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OHE for the test set only, takes X test dataframe and list of columns to encoded:\n",
    "def trans_ohe(X_dataframe, columns):\n",
    "    dums = ohe.transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = trans_ohe(X_test, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer_transform_only(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed = imputer_transform_only(soc_eco_h_i_imputer_knn, X_test_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have a working dataset of: \n",
    "    'X_train_imputed' and 'y_train' to fit models to, 'X_test_ohe' to generate predictions, and 'y_test' to validate models with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.783521\n",
       "1    0.216479\n",
       "Name: h1n1_vaccine, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.h1n1_vaccine.value_counts(normalize=True) # =>change to DummyClassifier later  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.531887\n",
       "1    0.468113\n",
       "Name: seasonal_vaccine, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.seasonal_vaccine.value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with best params (by grid search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1N1 cv-mean ROC_AUC score (All Binary/Best Params):0.8396370377897892\n",
      "Seasonal cv-mean ROC_AUC score (All Binary/Best Params):0.8564573211066776\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# H1N1 \n",
    "# {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_h1n1=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "logreg_h1n1.fit(X_train_imputed, y_train.h1n1_vaccine)\n",
    "\n",
    "h1n1_bp = cross_val_score(estimator=logreg_h1n1, X=X_train_imputed, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal \n",
    "#{'C': 1.0, 'class_weight': 'balanced', 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_sea=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=10000,\n",
    "                              solver='saga')\n",
    "\n",
    "sea_bp = cross_val_score(estimator=logreg_sea, X=X_train_imputed, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "\n",
    "print(f'H1N1 cv-mean ROC_AUC score (All Binary/Best Params):{h1n1_bp}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score (All Binary/Best Params):{sea_bp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-396ce9a6f6d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogreg_h1n1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_imputed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh1n1_vaccine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlogreg_h1n1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1466\u001b[0m                                                 self.solver == 'liblinear')))\n\u001b[1;32m   1467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0movr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\n",
    "logreg_h1n1=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "logreg_h1n1.fit(X_train_imputed, y_train.h1n1_vaccine)\n",
    "\n",
    "logreg_h1n1.predict_proba(X_test_ohe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_score, label_name, ax):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax.set_ylabel('TPR')\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_title(\n",
    "        f\"{label_name}: AUC = {roc_auc_score(y_true, y_score):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c9acdd9c4a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract the probabilitiy predictions for the \"1\" class (heart disease)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_hat_hd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get the FPR and TPR data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_hd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_prob' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the probabilitiy predictions for the \"1\" class (heart disease)\n",
    "y_hat_hd = y_prob[:, 1]\n",
    "\n",
    "# Get the FPR and TPR data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_hat_hd)\n",
    "\n",
    "# Plot the FPR and TPR data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)\n",
    "ax.plot((0,1), (0,1), 'k--')\n",
    "#print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test['h1n1_vaccine'], y_score)\n",
    "ax.plot(fpr, tpr)\n",
    "ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_xlabel('FPR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "\n",
    "plot_roc(\n",
    "    y_test['h1n1_vaccine'], \n",
    "    preds_log, \n",
    "    'h1n1_vaccine',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\n",
    "    y_eval['seasonal_vaccine'], \n",
    "    y_preds['seasonal_vaccine'], \n",
    "    'seasonal_vaccine',\n",
    "    ax=ax[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
