{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Codebook - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes grid search codes for  our logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------Run this part to create our dataframe---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('../Data/training_set_features.csv', index_col=\"respondent_id\")\n",
    "labels_df = pd.read_csv('../Data/training_set_labels.csv', index_col=\"respondent_id\")\n",
    "joined_df = features_df.join(labels_df, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an all-in-one data cleaning function. Do this BEFORE OHE\n",
    "# Maybe this should be a class and worked into the pipeline?\n",
    "def datacleaner(maindataframe):\n",
    "    #For dropping whole columns \n",
    "    def columndrop(dataframe, column_list):\n",
    "        dataframe.drop(column_list, axis = 1, inplace=True)\n",
    "    #For dropping rows with na values\n",
    "    def basicdropna(dataframe, column_list):\n",
    "        dataframe.dropna(subset=column_list, inplace=True)\n",
    "    #For special case imputation\n",
    "    def impute_missing_data(dataframe, column_list, fillvalue):\n",
    "        for column in column_list:\n",
    "            dataframe[column].fillna(fillvalue, inplace = True)\n",
    "    #This creates a number of lists of columns that fall into a few different \n",
    "    #categories, that will be processed in different ways. See notes below on how\n",
    "    #these choices were made.\n",
    "    drop_columns =  ['employment_industry',  'employment_occupation', 'hhs_geo_region']       \n",
    "        \n",
    "    general_dropna = ['health_worker', 'education','income_poverty', 'marital_status', \n",
    "                    'rent_or_own', 'employment_status', 'household_adults', \n",
    "                    'household_children' ]\n",
    "        \n",
    "    survey_col = ['opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc',\n",
    "         'opinion_seas_vacc_effective', 'opinion_seas_risk','opinion_seas_sick_from_vacc']\n",
    "\n",
    "    behavior_col = ['behavioral_antiviral_meds', 'behavioral_face_mask',\n",
    "                'behavioral_large_gatherings','behavioral_outside_home']\n",
    "\n",
    "    behavior_col_2 = ['behavioral_avoidance', \n",
    "                'behavioral_wash_hands','behavioral_touch_face']\n",
    "\n",
    "    doc_rec = ['doctor_recc_h1n1','doctor_recc_seasonal']\n",
    "    \n",
    "    basicdropna(maindataframe, general_dropna)\n",
    "    columndrop(maindataframe, drop_columns)\n",
    "    impute_missing_data(maindataframe, survey_col, 3)\n",
    "    impute_missing_data(maindataframe, ['h1n1_concern'], 2)\n",
    "    impute_missing_data(maindataframe, ['h1n1_knowledge'], 0)\n",
    "    impute_missing_data(maindataframe, behavior_col, 0)\n",
    "    impute_missing_data(maindataframe, behavior_col_2, 1)\n",
    "    impute_missing_data(maindataframe, doc_rec, 0)\n",
    "    impute_missing_data(maindataframe, ['chronic_med_condition'], 0)\n",
    "    impute_missing_data(maindataframe, ['child_under_6_months'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaner(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=joined_df.drop(['h1n1_vaccine','seasonal_vaccine'], axis=1)\n",
    "y=joined_df[['h1n1_vaccine','seasonal_vaccine']]\n",
    "\n",
    "# Train test split, do this before OHE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OHE for objects, do this before imputer\n",
    "\n",
    "cat_col_list = [i for i in X_train.select_dtypes(include='object').columns]\n",
    "\n",
    "nb_list_for_ohe = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "'opinion_seas_risk', 'opinion_seas_sick_from_vacc']\n",
    "\n",
    "# Fits OHE on a subset of columns, then reintegrates them into the\n",
    "# Origional dataframe. Do this after initial cleaning, before \n",
    "# health insurace imputation.\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "def fit_trans_ohe(X_dataframe, columns):\n",
    "        \n",
    "    dums = ohe.fit_transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated\n",
    "\n",
    "#We should end up with a fitted ohe instance called 'ohe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = fit_trans_ohe(X_train, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "socio_economic_column_list = [\"x0_35 - 44 Years\",\"x0_45 - 54 Years\",\"x0_55 - 64 Years\",\"x0_65+ Years\",\n",
    "                              \"x1_< 12 Years\",\"x1_College Graduate\",\"x1_Some College\",\"x2_Hispanic\",\n",
    "                              \"x2_Other or Multiple\",\"x2_White\",\"x3_Male\", \"x4_> $75,000\", \"x4_Below Poverty\",\n",
    "                              \"x5_Not Married\", \"x6_Rent\", \"x7_Not in Labor Force\",\"x7_Unemployed\",\n",
    "                              \"x8_MSA, Principle City\",'x8_Non-MSA', 'health_insurance']\n",
    "\n",
    "# Fitting an imputer for Health Insurance using socio-economic features, \n",
    "# pulling from a dataframe that has already been OneHotEncoded\n",
    "\n",
    "\n",
    "soc_eco_h_i_imputer_knn = KNNImputer()\n",
    "\n",
    "def soc_eco_KNN_imputer(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.fit_transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round() \n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = soc_eco_KNN_imputer(soc_eco_h_i_imputer_knn, X_train_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OHE for the test set only, takes X test dataframe and list of columns to encoded:\n",
    "def trans_ohe(X_dataframe, columns):\n",
    "    dums = ohe.transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = trans_ohe(X_test, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer_transform_only(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed = imputer_transform_only(soc_eco_h_i_imputer_knn, X_test_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have a working dataset of: \n",
    "    'X_train_imputed' and 'y_train' to fit models to, 'X_test_imputed' to generate predictions, and 'y_test' to validate models with.\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For H1N1 vaccination model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C, penalty, max_iter, solver, class_weight   \n",
    "\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\", 'elasticnet', 'none'],\n",
    "     \"max_iter\":[100, 1000, 10000, 100000, 1000000], \n",
    "      'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga' ],\n",
    "     \"class_weight\":['balanced', None]} # l1 lasso l2 ridge\n",
    "\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=5, scoring='roc_auc')\n",
    "logreg_cv.fit(X_train_ohe,y_train.h1n1_vaccine)\n",
    "\n",
    "print(\"Tuned hpyerparameters(best parameters) for H1N1 model: \",logreg_cv.best_params_)\n",
    "print(\"ROC_AUC for H1N1 model :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Seasonal vaccination model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C, penalty, max_iter, solver, class_weight   \n",
    "\n",
    "grid_s={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\", 'elasticnet', 'none'],\n",
    "     \"max_iter\":[100, 1000, 10000, 100000, 1000000], \n",
    "      'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga' ],\n",
    "     \"class_weight\":['balanced', None]} # l1 lasso l2 ridge\n",
    "\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv_s=GridSearchCV(logreg,grid_s,cv=5, scoring='roc_auc')\n",
    "logreg_cv_s.fit(X_train_ohe,y_train.seasonal_vaccine)\n",
    "\n",
    "\n",
    "print(\"Tuned hpyerparameters(best parameters) for Seasonal Flu model: \",logreg_cv_s.best_params_)\n",
    "print(\"ROC_AUC for Seasonal Flu model :\",logreg_cv_s.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with a default model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Model\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# H1N1 \n",
    "h1n1 = cross_val_score(estimator=lr, X=X_train_ohe, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal\n",
    "sea = cross_val_score(estimator=lr, X=X_train_ohe, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "print(\"ROC_AUC for H1N1 model :\", h1n1)\n",
    "print()\n",
    "print(\"ROC_AUC for H1N1 model :\",logreg_cv.best_score_)\n",
    "print()\n",
    "print(\"ROC_AUC for Seasonal Flu model :\", sea)\n",
    "print()\n",
    "print(\"ROC_AUC for Seasonal Flu model :\",logreg_cv_s.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# H1N1 \n",
    "# {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_h1n1=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=1000000,\n",
    "                              solver='saga')\n",
    "\n",
    "logreg_h1n1.fit(X_train_imputed, y_train.h1n1_vaccine)\n",
    "\n",
    "h1n1_bp = cross_val_score(estimator=logreg_h1n1, X=X_train_imputed, y=y_train.h1n1_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "# Seasonal \n",
    "#{'C': 1.0, 'class_weight': 'balanced', 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "\n",
    "logreg_sea=LogisticRegression(C=1,penalty=\"l1\", class_weight='balanced', max_iter=10000,\n",
    "                              solver='saga')\n",
    "\n",
    "sea_bp = cross_val_score(estimator=logreg_sea, X=X_train_imputed, y=y_train.seasonal_vaccine, \n",
    "                cv=5, scoring='roc_auc').mean() \n",
    "\n",
    "\n",
    "print(f'H1N1 cv-mean ROC_AUC score (All Binary/Best Params):{h1n1_bp}')\n",
    "print(f'Seasonal cv-mean ROC_AUC score (All Binary/Best Params):{sea_bp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
