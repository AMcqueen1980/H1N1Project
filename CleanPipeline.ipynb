{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('Data/training_set_features.csv', index_col=\"respondent_id\")\n",
    "labels_df = pd.read_csv('Data/training_set_labels.csv', index_col=\"respondent_id\")\n",
    "joined_df = features_df.join(labels_df, how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A detailed overview and explanation of our data cleaning steps:\n",
    "\n",
    "The following steps were taken to clean our database.\n",
    "\n",
    "First we dropped the columns of employment_industry and employment_occupation. Both columns showed a high number of null datapoints with no good way to impute data. Rather than lose 13330 rows of data, we decided to lose these two columns and let the rest of our socio-economic data stand in as proxy.\n",
    "\n",
    "Next we faced categories with a scattering of data with no real way to impute information. We simply dropped rows with nulls in these columns, slightly (but only slightly) reducing our data set.\n",
    "\n",
    "Then we created a few special case imputations for some of our data. Most of these columns relate to responses to survery data. Here we imputed default answers as made sense. For instance, if someone fails to answer for whether or not they have a chronic medical condition, we assume that they do not. If a condition was severe enough to be chronic, we feel that a respondent would be aware of that enough to mark yes. These assumptions won't ne right all the time, but we believe that they'll still add to the predictive power of the model. \n",
    "\n",
    "The decisions imputations made are as follows:\n",
    "\n",
    "* Blanks on survey questions were imputed to the 'neutral' or 'don't know' values.\n",
    "\n",
    "* In behavioral columns we imputed with the majority class. \n",
    "\n",
    "* We assumed that if a patient had a doctor's recommendation they would say so.\n",
    "\n",
    "* And finally was assumed that if a respondent had close contact with a child under 6 months they would know that, and we imputed a no reseponse.\n",
    "\n",
    "After this data cleaning we are left with a large number of null values in the health insurance column. We felt that this factor was to important to drop, as we had been forced to for employment_industry, for instance. Instead we decided to impute that data via KNN imputation. However, that will have to wait until after we've split the test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an all-in-one data cleaning function. Do this BEFORE OHE\n",
    "# Maybe this should be a class and worked into the pipeline?\n",
    "def datacleaner(maindataframe):\n",
    "    #For dropping whole columns \n",
    "    def columndrop(dataframe, column_list):\n",
    "        dataframe.drop(column_list, axis = 1, inplace=True)\n",
    "    #For dropping rows with na values\n",
    "    def basicdropna(dataframe, column_list):\n",
    "        dataframe.dropna(subset=column_list, inplace=True)\n",
    "    #For special case imputation\n",
    "    def impute_missing_data(dataframe, column_list, fillvalue):\n",
    "        for column in column_list:\n",
    "            dataframe[column].fillna(fillvalue, inplace = True)\n",
    "    #This creates a number of lists of columns that fall into a few different \n",
    "    #categories, that will be processed in different ways. See notes below on how\n",
    "    #these choices were made.\n",
    "    drop_columns =  ['employment_industry',  'employment_occupation', 'hhs_geo_region' ]       \n",
    "        \n",
    "    general_dropna = ['health_worker', 'education','income_poverty', 'marital_status', \n",
    "                    'rent_or_own', 'employment_status', 'household_adults', \n",
    "                    'household_children' ]\n",
    "        \n",
    "    survey_col = ['opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc',\n",
    "         'opinion_seas_vacc_effective', 'opinion_seas_risk','opinion_seas_sick_from_vacc']\n",
    "\n",
    "    behavior_col = ['behavioral_antiviral_meds', 'behavioral_face_mask',\n",
    "                'behavioral_large_gatherings','behavioral_outside_home']\n",
    "\n",
    "    behavior_col_2 = ['behavioral_avoidance', \n",
    "                'behavioral_wash_hands','behavioral_touch_face']\n",
    "\n",
    "    doc_rec = ['doctor_recc_h1n1','doctor_recc_seasonal']\n",
    "    \n",
    "    basicdropna(maindataframe, general_dropna)\n",
    "    columndrop(maindataframe, drop_columns)\n",
    "    impute_missing_data(maindataframe, survey_col, 3)\n",
    "    impute_missing_data(maindataframe, ['h1n1_concern'], 2)\n",
    "    impute_missing_data(maindataframe, ['h1n1_knowledge'], 0)\n",
    "    impute_missing_data(maindataframe, behavior_col, 0)\n",
    "    impute_missing_data(maindataframe, behavior_col_2, 1)\n",
    "    impute_missing_data(maindataframe, doc_rec, 0)\n",
    "    impute_missing_data(maindataframe, ['chronic_med_condition'], 0)\n",
    "    impute_missing_data(maindataframe, ['child_under_6_months'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaner(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=joined_df.drop(['h1n1_vaccine','seasonal_vaccine'], axis=1)\n",
    "y=joined_df[['h1n1_vaccine','seasonal_vaccine']]\n",
    "\n",
    "# Train test split, do this before OHE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OHE for objects, do this before imputer\n",
    "\n",
    "#This is a list of columns that are obviously non-numeric\n",
    "cat_col_list = [i for i in X_train.select_dtypes(include='object').columns]\n",
    "\n",
    "#These columns are numeric, but correspond to non-numeric responses on survey data.\n",
    "nb_list_for_ohe = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "'opinion_seas_risk', 'opinion_seas_sick_from_vacc']\n",
    "\n",
    "# Fits OHE on a subset of columns, then reintegrates them into the\n",
    "# Origional dataframe. Do this after initial cleaning, before \n",
    "# health insurace imputation.\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "def fit_trans_ohe(X_dataframe, columns):\n",
    "        \n",
    "    dums = ohe.fit_transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated\n",
    "\n",
    "#We should end up with a fitted ohe instance called 'ohe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = fit_trans_ohe(X_train, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We created a subset list for obviously socio-economic data.    \n",
    "socio_economic_column_list = [\"x0_35 - 44 Years\",\"x0_45 - 54 Years\",\"x0_55 - 64 Years\",\"x0_65+ Years\",\n",
    "                              \"x1_< 12 Years\",\"x1_College Graduate\",\"x1_Some College\",\"x2_Hispanic\",\n",
    "                              \"x2_Other or Multiple\",\"x2_White\",\"x3_Male\", \"x4_> $75,000\", \"x4_Below Poverty\",\n",
    "                              \"x5_Not Married\", \"x6_Rent\", \"x7_Not in Labor Force\",\"x7_Unemployed\",\n",
    "                              \"x8_MSA, Principle City\",'x8_Non-MSA', 'health_insurance']\n",
    "\n",
    "\n",
    "# Fitting an imputer for Health Insurance using socio-economic features, \n",
    "# pulling from a TRAINING dataframe that has already been OneHotEncoded.\n",
    "soc_eco_h_i_imputer_knn = KNNImputer()\n",
    "\n",
    "def soc_eco_KNN_imputer(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.fit_transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round() \n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = soc_eco_KNN_imputer(soc_eco_h_i_imputer_knn, X_train_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training data treated, we move on to the test data. Everything done in this section should be transforming only, not fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies the OHE for the test set only, takes X test dataframe and list of columns to encoded:\n",
    "def trans_ohe(X_dataframe, columns):\n",
    "    dums = ohe.transform(X_dataframe[columns])\n",
    "    dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_dataframe.index)\n",
    "    df_cat_dropped = X_dataframe.drop(cat_col_list, axis = 1)\n",
    "    dums_df_concated = pd.concat([df_cat_dropped, dums_df], axis=1)\n",
    "    return dums_df_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = trans_ohe(X_test, cat_col_list+nb_list_for_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer_transform_only(imputer, dataframe, column_list):\n",
    "    soc_econ_base = dataframe[column_list]\n",
    "    soc_econ_imputed = pd.DataFrame(imputer.transform(soc_econ_base), \n",
    "                                         columns = soc_econ_base.columns,\n",
    "                                        index=soc_econ_base.index)\n",
    "    remainder_df = dataframe.drop(column_list, axis = 1)\n",
    "    output_df = remainder_df.join(soc_econ_imputed)\n",
    "    output_df.health_insurance = output_df.health_insurance.round()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed = imputer_transform_only(soc_eco_h_i_imputer_knn, X_test_ohe, socio_economic_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have a working dataset of: \n",
    "    'X_train_imputed' and 'y_train' to fit models to, 'X_test_imputed' to generate test predictions, and 'y_test' to validate models with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
